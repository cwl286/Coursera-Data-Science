---
title: "Capstone Project Milestone Report"
author: "Chris Gomes"
date: "August 29, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load libraries}
library(ggplot2)
library(stringi)
library(tm)
library(wordcloud)
```

## Abstract

We download the data sets that will be used to construct a prediction app. We clean the data and then perform some exploratory data analysis. We begin to think about how to build the algorithm for 
our app.

## Data Processing

### Download the Data Sets

The data is stored in a zip file which may be downloaded [here](https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip).

```{r download, cache=TRUE}
src_file <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
dest_file <- "capstone-data.zip"

# Download and extract the files
download.file(src_file, dest_file)
unzip(dest_file)
```

Let's see which files we've downloaded.
```{r inspect the extracted files, cache=TRUE}
unzip(dest_file, list = TRUE )
```

We consider only the Enlish language files.
```{r inspect the English language files, cache=TRUE}
list.files("final")
list.files("final/en_US")
```

### Save the English Language Files

We save the text data to three files: `blogs`, `news`, and `twitter`.
```{r Create the files blogs, news, and twitter, cache=TRUE}
blogs <- readLines("final/en_US/en_US.blogs.txt", encoding = "UTF-8", skipNul=TRUE)
news <- readLines("final/en_US/en_US.news.txt", encoding = "UTF-8", skipNul=TRUE)
twitter <- readLines("final/en_US/en_US.twitter.txt", encoding = "UTF-8", skipNul=TRUE)

# save the data to an .RData files
save(blogs, file="blogs.RData")
save(news, file="news.RData")
save(twitter, file="twitter.RData")
```


## Basic Statistics

First, we look at properties of the files themselves.

```{r file properties, cache=TRUE}
# Check the files sizes
blogs_size <- file.info("final/en_US/en_US.blogs.txt")$size / 1024.0 / 1024.0
news_size <- file.info("final/en_US/en_US.news.txt")$size / 1024.0 / 1024.0
twitter_size <- file.info("final/en_US/en_US.twitter.txt")$size / 1024.0 / 1024.0

# Use stringi to get data about numbers of lines and word counts for each file
stri_stats_general(blogs)
stri_stats_general(news)
stri_stats_general(twitter)

# word counts
blog_words <- stri_count_words(blogs)
news_words <- stri_count_words(news)
twitter_words <- stri_count_words(twitter)
```


We look at summaries and word distributions for each file.

```{r summaries and plots}
summary(blog_words)
qplot(blog_words, binwidth=1, xlim=c(1, 100))

summary(news_words)
qplot(news_words, binwidth=1, xlim=c(1, 100))

summary(twitter_words)
qplot(twitter_words, binwidth=1, xlim=c(1, 30))
```

### Data Sampling

Given the large  sizes of these files, we sample 10,000 lines from each of them in order to improve data processing efficiency.

```{r create sample file, cache=TRUE}
blogs_samp <- blogs[sample(1:length(blogs),10000)]
news_samp <- news[sample(1:length(news),10000)]
twitter_samp <- twitter[sample(1:length(twitter),10000)]
all_samp <- c(blogs_samp, news_samp, twitter_samp)

# Save the sampled data to a .txt files
#save(all_samp, file="all_samp.txt")
writeLines(all_samp, "./sample/all_samp.txt")

# Statistics for the sample
stri_stats_general(all_samp)
samp_words <- stri_count_words(all_samp)

summary(samp_words)
qplot(samp_words, binwidth=1, xlim=c(1, 100))
```

## Data Cleaning

It is from this sample that we construct our corpus. WE use the `tm` (text mining) library to perform the following transformations:

* Convert all words to lower case
* Strip away all white space
* Strip away all punctuation
* Strip away all numbers
* Strip way various non-alphanumeric characters
* Remove stop words
* Strip away all urls

```{r data cleaning, eval=FALSE}
cname <- file.path("")
corp <- Corpus(DirSource(cname))
#corp <- Corpus("all_samp.RData")

# Perform the transformations
corp <- tm_map(corp, content_transformer(tolower))
corp <- tm_map(corp, stripWhitespace)
corp <- tm_map(corp, removePunctuation)
corp <- tm_map(corp, removeNumbers)
# The characters /, @, |, and # can appear in email and tweets
special_chars <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
corp <- tm_map(corp, special_chars, "#|/|@|\\|")

corp <- tm_map(corp, removeWords, stopwords("english"))

# See http://www.rdatamining.com/books/rdm/faq/removeurlsfromtext
removeURL <- function(x) gsub("http[[:alnum:][:punct:]]*", "", x) 
corp <- tm_map(corp, content_transformer(removeURL))

# Save the cleaned corpus
writeCorpus(corp)
```

