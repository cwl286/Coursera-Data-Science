---
title: "Capstone Project Milestone Report"
author: "Chris Gomes"
date: "November 19, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load libraries, echo=FALSE, message=FALSE}
library(ggplot2)
library(dplyr)
library(stringi)
library(tm)
library(wordcloud)
library(RWeka)
```

## Abstract
We complete the first steps towards constructing a prediction app for Coursera's Data Science capstone project. We download the data sets that will be used to train the app. We clean the data, construct corpora, and perform some exploratory data analysis. We begin to think about how to build the algorithm for our app.

For ease of reading, I have suppressed the display of most of the code.

## Data Processing

### Download the Data Sets

The data is stored in a zip file which may be downloaded [here](https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip).

```{r download, cache=TRUE, echo=FALSE}
src_file <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
dest_file <- "capstone-data.zip"

# Download and extract the files
download.file(src_file, dest_file)
unzip(dest_file)
```

Let's see which files we've downloaded.
```{r inspect the extracted files, cache=TRUE, echo=FALSE, results='hide'}
unzip(dest_file, list = TRUE )
```

We consider only the Enlish language files.
```{r inspect the English language files, cache=TRUE}
# list.files("final")
list.files("final/en_US")
```

```{r create data files, cache=TRUE, echo=FALSE}
#Read in blogs and twitter files
blogs <- readLines("final/en_US/en_US.blogs.txt", encoding = "UTF-8", skipNul=TRUE)
twitter <- readLines("final/en_US/en_US.twitter.txt", encoding = "UTF-8", skipNul=TRUE)

#Read the news file, using binary mode as there are special characters in the text
con<-file("final/en_US/en_US.news.txt", open="rb")
news<-readLines(con, encoding="UTF-8")
close(con)
rm(con)
```

### Convert all characters to ASCII and save to text files

This was necessary since the news file had characters (emoticons) that were causing the program to crash.
```{r eliminate non-Roman charaters, cache=TRUE}

blogs <- iconv(blogs, "latin1", "ASCII", sub="")
news <- iconv(news, "latin1", "ASCII", sub="")
twitter <- iconv(twitter, "latin1", "ASCII", sub="")

# save the data to .txt files
save(blogs, file="blogs.txt")
save(news, file="news.txt")
save(twitter, file="twitter.txt")
```


## Basic Statistics

First, we look at properties of the files themselves.

```{r file properties, cache=TRUE, echo=FALSE}
# Check the files sizes
blogs_size <- file.info("final/en_US/en_US.blogs.txt")$size / 1024.0^2
news_size <- file.info("final/en_US/en_US.news.txt")$size / 1024.0^2
twitter_size <- file.info("final/en_US/en_US.twitter.txt")$size / 1024.0^2

# line counts
blog_lines <- length(blogs)
news_lines <- length(news)
twitter_lines <- length(twitter)

# word counts
blog_words <- sum(stri_count_words(blogs))
news_words <- sum(stri_count_words(news))
twitter_words <- sum(stri_count_words(twitter))
```

```{r print table, cache=TRUE, echo=FALSE}
summary_table<-data.frame(Source=c("Blogs","Twitter", "News"),
                         Size_in_MB=c(round(blogs_size, digits = 2), round(twitter_size, digits = 2), round(news_size, digits = 2)),
                         Total_Lines=c(blog_lines, twitter_lines, news_lines),
                         Total_Words=c(blog_words, twitter_words, news_words)
                         )

summary_table
```

Get data about the line counts, character counts, and 5-number summary for words for each file.

For `blogs`, we have
```{r print blogs lines properties, cache=TRUE, echo=FALSE}
# Use stringi to get data about numbers of lines and word counts for each file
stri_stats_general(blogs)
summary(stri_count_words(blogs))
```
For `news`, we have
```{r print news lines properties, cache=TRUE, echo=FALSE}
# Use stringi to get data about numbers of lines and word counts for each file
stri_stats_general(news)
summary(stri_count_words(news))
```
For `twitter`, we have
```{r print twitter lines properties, cache=TRUE, echo=FALSE}
# Use stringi to get data about numbers of lines and word counts for each file
stri_stats_general(twitter)
summary(stri_count_words(twitter))
```



### Data Sampling

Given the large  sizes of these files, we sample 10,000 lines from each file in order to improve data processing efficiency. The resulting file is called `all_samp`.

```{r create sample file, cache=TRUE, echo=FALSE}
blogs_samp <- blogs[sample(1:length(blogs),10000)]
news_samp <- news[sample(1:length(news),10000)]
twitter_samp <- twitter[sample(1:length(twitter),10000)]
all_samp <- c(blogs_samp, news_samp, twitter_samp)
save(all_samp, file="all_samp.txt")
# Save the sampled data to a .txt files
writeLines(all_samp, "../sample/all_samp.txt")
```

```{r stats for sample file, cache=TRUE, echo=FALSE}
# Statistics for the sample
samp_size <- file.info("all_samp.txt")$size / 1024.0^2
samp_lines <- length(all_samp)
samp_words <- sum(stri_count_words(all_samp))

all_samp_table<-data.frame(Source=c("All Samples"),
                         Size_in_MB=c(round(samp_size, digits = 2)),
                         Total_Lines=c(samp_lines),
                         Total_Words=c(samp_words)
                         )

all_samp_table

stri_stats_general(all_samp)
summary(stri_count_words(all_samp))
```


## Data Cleaning and Corpus Building

We create a corpus from the `all_samp.txt` file and then clean it. We use the text mining library `tm` to perform the following transformations:

* Convert all words to lower case
* Strip away all white space
* Strip away all punctuation
* Strip away all numbers
* Strip way various non-alphanumeric characters
* Remove stop words (i.e. words that are uninteresting, but appear frequently in text such as "the","and", "also", ... etc.)
* Strip away all urls
* Remove profanity
* Stemming to remove common word endings (e.g. ''s', 'ing', ... etc)

```{r data cleaning, cache=FALSE, echo=FALSE}
corp <- Corpus(VectorSource(list(all_samp)))

# Perform the transformations
corp <- tm_map(corp, content_transformer(tolower))
corp <- tm_map(corp, stripWhitespace)
corp <- tm_map(corp, removePunctuation)
corp <- tm_map(corp, removeNumbers)
# The characters /, @, |, and # can appear in email and tweets
special_chars <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
corp <- tm_map(corp, special_chars, "#|/|@|\\|")

corp <- tm_map(corp, removeWords, stopwords("english"))

# See http://www.rdatamining.com/books/rdm/faq/removeurlsfromtext
removeURL <- function(x) gsub("http[[:alnum:][:punct:]]*", "", x) 
corp <- tm_map(corp, content_transformer(removeURL))

# Remove profanities
# The list used can be found here:  http://www.freewebheaders.com/full-list-of-bad-words-banned-by-google/ 

profanities <- read.delim("bad-words-banned-by-google.txt",sep = ":",header = FALSE)
profanities <- profanities[,1]
corp <- tm_map(corp, removeWords, profanities)

# Stemming
library(SnowballC)   
corp <- tm_map(corp, stemDocument) 

corp <- tm_map(corp, PlainTextDocument) 
 
```



## N-Gram Tokenization

We use unigrams, bigrams, and trigrams to find word frequencies and correlations bewtween words.

```{r n-gram tokenization, cache=TRUE}
# For more information, see: https://cran.r-project.org/web/packages/tm/vignettes/tm.pdf

dtm <- DocumentTermMatrix(corp)
dtm <- removeSparseTerms(dtm, 0.75)

uni_tokenizer <- function(x) NGramTokenizer(corp, Weka_control(min = 1, max = 1))
unidtm <- DocumentTermMatrix(corp, control = list(tokenize = uni_tokenizer))

bi_tokenizer <- function(x) NGramTokenizer(corp, Weka_control(min = 2, max = 2))
bidtm <- DocumentTermMatrix(corp, control = list(tokenize = bi_tokenizer))

tri_tokenizer <- function(x) NGramTokenizer(corp, Weka_control(min = 3, max = 3))
tridtm <- DocumentTermMatrix(corp, control = list(tokenize = tri_tokenizer))
```

```{r create transpose of tdm, cache=TRUE, echo=FALSE}
tdm <- TermDocumentMatrix(corp)
tdm<- removeSparseTerms(tdm, 0.75)
```

## Exploratory Data Analysis

We will use histograms and word clouds to explore the frequencies of words in our corpus.
Let us start by looking a words with high frequency:

```{r organize words by frequency, cache=TRUE, echo=FALSE}
unifreq <- sort(colSums(as.matrix(unidtm)), decreasing=TRUE)
uniwordfreq <- data.frame(word=names(unifreq), freq=unifreq)
paste("Unigrams - Top 5 highest frequencies")
head(uniwordfreq,5)
```


```{r plot most common words, cache=TRUE, echo=FALSE}
uniwordfreq %>% 
    filter(freq > 1000) %>%
    ggplot(aes(word,freq)) +
    geom_bar(stat="identity") +
    ggtitle("Unigrams with frequencies > 1000") +
    xlab("Unigrams") + ylab("Frequency") +
    theme(axis.text.x=element_text(angle=45, hjust=1))
```

