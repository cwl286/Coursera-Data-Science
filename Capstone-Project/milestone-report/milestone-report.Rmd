---
title: "Capstone Project Milestone Report"
author: "Chris Gomes"
date: "November 19, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load libraries, echo=FALSE, message=FALSE}
library(ggplot2)
library(stringi)
library(tm)
library(wordcloud)
library(RWeka)
```

## Abstract
We complete the first steps towards constructing a prediction app for Coursera's Data Science capstone project. We download the data sets that will be used to train the app. We clean the data, construct corpora, and perform some exploratory data analysis. We begin to think about how to build the algorithm for our app.

For ease of reading, I have suppressed the display of most of the code.

## Data Processing

### Download the Data Sets

The data is stored in a zip file which may be downloaded [here](https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip).

```{r download, cache=TRUE, echo=FALSE}
src_file <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
dest_file <- "capstone-data.zip"

# Download and extract the files
download.file(src_file, dest_file)
unzip(dest_file)
```

Let's see which files we've downloaded.
```{r inspect the extracted files, cache=TRUE, echo=FALSE, results='hide'}
unzip(dest_file, list = TRUE )
```

We consider only the Enlish language files.
```{r inspect the English language files, cache=TRUE}
# list.files("final")
list.files("final/en_US")
```

```{r create data files, cache=TRUE}
#Read in blogs and twitter files
blogs <- readLines("final/en_US/en_US.blogs.txt", encoding = "UTF-8", skipNul=TRUE)
twitter <- readLines("final/en_US/en_US.twitter.txt", encoding = "UTF-8", skipNul=TRUE)

#Read the news file, using binary mode as there are special characters in the text
con<-file("final/en_US/en_US.news.txt", open="rb")
news<-readLines(con, encoding="UTF-8")
close(con)
rm(con)
```

### Convert all characters to ASCII and save to text files

This was necessary since the news file had characters (emoticons) that were causing the program to crash.
```{r eliminate non-Roman charaters, cache=TRUE}

blogs <- iconv(blogs, "latin1", "ASCII", sub="")
news <- iconv(news, "latin1", "ASCII", sub="")
twitter <- iconv(twitter, "latin1", "ASCII", sub="")

# save the data to .txt files
save(blogs, file="blogs.txt")
save(news, file="news.txt")
save(twitter, file="twitter.txt")
```


## Basic Statistics

First, we look at properties of the files themselves.

```{r file properties, cache=TRUE, echo=FALSE}
# Check the files sizes
blogs_size <- file.info("final/en_US/en_US.blogs.txt")$size / 1024.0^2
news_size <- file.info("final/en_US/en_US.news.txt")$size / 1024.0^2
twitter_size <- file.info("final/en_US/en_US.twitter.txt")$size / 1024.0^2

# line counts
blog_lines <- length(blogs)
news_lines <- length(news)
twitter_lines <- length(twitter)

# word counts
blog_words <- sum(stri_count_words(blogs))
news_words <- sum(stri_count_words(news))
twitter_words <- sum(stri_count_words(twitter))
```

```{r print table, cache=TRUE, echo=FALSE}
summary_table<-data.frame(source=c("Blogs","Twitter", "News"),
                         Size_in_MB=c(blogs_size, twitter_size, news_size),
                         Total_Lines=c(blog_lines, twitter_lines, news_lines),
                         Total_Words=c(blog_words, twitter_words, news_words)
                         )

summary_table
```

Get data about the numbers of lines and word counts for each file.

For `blogs`, we have
```{r print blogs lines properties, cache=TRUE, echo=FALSE}
# Use stringi to get data about numbers of lines and word counts for each file
stri_stats_general(blogs)
stri_stats_general(news)
stri_stats_general(twitter)


```




We look at summaries and word distributions for each file.

```{r summaries and plots}
summary(blog_words)
qplot(blog_words, binwidth=1, xlim=c(1, 100))

summary(news_words)
qplot(news_words, binwidth=1, xlim=c(1, 100))

summary(twitter_words)
qplot(twitter_words, binwidth=1, xlim=c(1, 30))
```

### Data Sampling

Given the large  sizes of these files, we sample 10,000 lines from `blog` and 'news` and 20,000 lines from `twitter` in order to improve data processing efficiency.

```{r create sample file, cache=TRUE}
blogs_samp <- blogs[sample(1:length(blogs),10000)]
news_samp <- news[sample(1:length(news),10000)]
twitter_samp <- twitter[sample(1:length(twitter),10000)]
all_samp <- c(blogs_samp, news_samp, twitter_samp)
save(all_samp, file="all_samp.txt")
# Save the sampled data to a .txt files
writeLines(all_samp, "../sample/all_samp.txt")

# Statistics for the sample
stri_stats_general(all_samp)
samp_words <- stri_count_words(all_samp)

summary(samp_words)
qplot(samp_words, binwidth=1, xlim=c(1, 100))
```


## Data Cleaning and Corpus Building

We create a corpus from the `all_samp.txt` file and then clean it. We use the text mining library `tm` to perform the following transformations:

* Convert all words to lower case
* Strip away all white space
* Strip away all punctuation
* Strip away all numbers
* Strip way various non-alphanumeric characters
* Remove stop words
* Strip away all urls
* Remove profanity

```{r data cleaning, cache=FALSE}
corp <- Corpus(VectorSource(list(all_samp)))

# Perform the transformations
corp <- tm_map(corp, content_transformer(tolower))
corp <- tm_map(corp, stripWhitespace)
corp <- tm_map(corp, removePunctuation)
corp <- tm_map(corp, removeNumbers)
# The characters /, @, |, and # can appear in email and tweets
special_chars <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
corp <- tm_map(corp, special_chars, "#|/|@|\\|")

corp <- tm_map(corp, removeWords, stopwords("english"))

# See http://www.rdatamining.com/books/rdm/faq/removeurlsfromtext
removeURL <- function(x) gsub("http[[:alnum:][:punct:]]*", "", x) 
corp <- tm_map(corp, content_transformer(removeURL))

# Remove profanities
# The list used can be found here:  http://www.freewebheaders.com/full-list-of-bad-words-banned-by-google/ 

profanities <- read.delim("bad-words-banned-by-google.txt",sep = ":",header = FALSE)
profanities <- profanities[,1]
corp <- tm_map(corp, removeWords, profanities)

# Save the cleaned corpus
#writeCorpus(corp, path = "../sample", filenames="corp.txt")
#corp <- readLines("corp.txt")
 
```


## N-Gram Tokenization

We use unigrams, bigrams, and trigrams to find word frequencies and astrong associations (i.e. correlations) bewtween words.

```{r n-gram tokenization, cache=TRUE}
# For more information, see: https://cran.r-project.org/web/packages/tm/vignettes/tm.pdf

tdm <- TermDocumentMatrix(corp)
tdm <- removeSparseTerms(tdm, 0.75)
# inspect(tdm)

uni <- NGramTokenizer(corp, Weka_control(min = 1, max = 1))
str(uni)

bi <- NGramTokenizer(corp, Weka_control(min = 2, max = 2))
str(bi)

tri <- NGramTokenizer(corp, Weka_control(min = 3, max = 3))
str(tri)
```

